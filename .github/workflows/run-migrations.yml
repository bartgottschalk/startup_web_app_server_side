# ============================================================================
# GitHub Actions Workflow: Run Database Migrations on AWS RDS
# ============================================================================
#
# PURPOSE:
# This workflow automates database migrations for the StartupWebApp multi-tenant
# RDS instance. It runs tests, builds a Docker image, pushes it to AWS ECR,
# and executes migrations via ECS Fargate.
#
# WORKFLOW OVERVIEW:
# 1. Manual trigger with database selection (safety feature)
# 2. Run full test suite (740 tests) to ensure code quality
# 3. Build production Docker image (multi-stage Dockerfile)
# 4. Push image to AWS ECR (container registry)
# 5. Run ECS Fargate task to execute "python manage.py migrate"
# 6. Stream CloudWatch logs to see migration results
#
# WHEN TO USE:
# - After merging code with new Django migrations
# - When setting up a new database for a fork
# - When you need to apply pending migrations to production
#
# REQUIRED GITHUB SECRETS:
# - AWS_ACCESS_KEY_ID: IAM user access key with ECR/ECS permissions
# - AWS_SECRET_ACCESS_KEY: IAM user secret access key
# - AWS_REGION: us-east-1 (or your AWS region)
#
# ============================================================================

# Workflow name (appears in GitHub Actions UI)
name: Run Database Migrations

# ============================================================================
# TRIGGER CONFIGURATION
# ============================================================================
#
# workflow_dispatch: Allows manual triggering from GitHub UI
# This is intentional - migrations should NOT run automatically on every push.
# Database changes are sensitive and should be triggered deliberately.
#
on:
  workflow_dispatch:
    # inputs: Define parameters users can provide when triggering
    inputs:
      database:
        description: 'Target Database'
        required: true
        type: choice
        # These options appear as a dropdown in GitHub UI
        options:
          - startupwebapp_prod
          - healthtech_experiment
          - fintech_experiment
        default: 'startupwebapp_prod'

      skip_tests:
        description: 'Skip test suite (use only for urgent hotfixes)'
        required: false
        type: boolean
        default: false

      skip_unit_tests:
        description: 'Skip unit tests (for faster iteration when debugging functional tests)'
        required: false
        type: boolean
        default: false

# ============================================================================
# ENVIRONMENT VARIABLES
# ============================================================================
#
# These variables are available to all jobs in the workflow.
# They're pulled from GitHub Secrets (encrypted storage) or workflow inputs.
#
env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: startupwebapp-backend
  ECS_CLUSTER: startupwebapp-cluster
  ECS_TASK_DEFINITION: startupwebapp-migration-task
  DB_SECRET_NAME: rds/startupwebapp/multi-tenant/master
  # The database name comes from user input
  DATABASE_NAME: ${{ github.event.inputs.database }}

# ============================================================================
# JOBS
# ============================================================================
#
# Jobs define the major stages of work. Each job runs on a separate virtual
# machine (called a "runner"). Jobs can run in parallel or sequentially.
#

jobs:
  # ==========================================================================
  # JOB 1: Run Tests
  # ==========================================================================
  #
  # PURPOSE: Validate code quality before deploying to production
  # RUNS ON: Ubuntu latest (GitHub-provided VM)
  # DURATION: ~5-7 minutes
  #
  # WHY: We never want to deploy broken code to production. Running tests
  # first ensures that all 740 tests pass before we touch the database.
  #
  test:
    name: Run Test Suite
    runs-on: ubuntu-latest

    # Skip this job if user selected skip_tests=true
    if: ${{ github.event.inputs.skip_tests != 'true' }}

    # Service containers run alongside the job
    # We need PostgreSQL for running tests
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: startupwebapp_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        # Health check: wait for PostgreSQL to be ready before starting tests
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      # ----------------------------------------------------------------------
      # STEP 1: Checkout code
      # ----------------------------------------------------------------------
      # Downloads your repository code to the runner VM
      - name: Checkout code
        uses: actions/checkout@v4

      # ----------------------------------------------------------------------
      # STEP 2: Set up Python
      # ----------------------------------------------------------------------
      # Installs Python 3.12 (matches your production environment)
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          # Cache pip packages for faster subsequent runs
          cache: 'pip'

      # ----------------------------------------------------------------------
      # STEP 3: Install dependencies
      # ----------------------------------------------------------------------
      # Installs all Python packages from requirements.txt
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ----------------------------------------------------------------------
      # STEP 4: Create test settings_secret.py
      # ----------------------------------------------------------------------
      # Create a minimal settings_secret.py for CI testing
      # This file is gitignored in local dev but needed for Django to load
      - name: Create settings_secret.py for CI
        run: |
          cat > StartupWebApp/StartupWebApp/settings_secret.py << 'EOF'
          # CI/CD Test Settings (auto-generated by GitHub Actions)
          # This file is temporary and only exists during the workflow run

          import os

          # Django secret key (for testing only - not production)
          SECRET_KEY = 'test-secret-key-for-ci-only-not-secure'

          # Allowed hosts for testing
          ALLOWED_HOSTS = [
              'localhost',
              'backend',
              'testserver',
              'localliveservertestcase.startupwebapp.com',
              'localliveservertestcaseapi.startupwebapp.com',
          ]

          # Security settings (disabled for testing)
          SESSION_COOKIE_SECURE = False
          SESSION_COOKIE_DOMAIN = None
          CSRF_COOKIE_SECURE = False
          CSRF_COOKIE_DOMAIN = ".startupwebapp.com"  # Share CSRF cookies across subdomains for functional tests
          CSRF_TRUSTED_ORIGINS = [
              'http://localhost',
              'http://testserver',
              'http://localliveservertestcase.startupwebapp.com',
              'http://localliveservertestcaseapi.startupwebapp.com',
          ]

          DEBUG = True
          ENVIRONMENT_DOMAIN = 'http://localhost'

          # Email configuration (console backend for testing)
          EMAIL_HOST = 'localhost'
          EMAIL_PORT = 1025
          EMAIL_USE_TLS = False
          EMAIL_HOST_USER = ''
          EMAIL_HOST_PASSWORD = ''
          EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend'

          # Database configuration from environment variables
          DATABASES = {
              'default': {
                  'ENGINE': 'django.db.backends.postgresql',
                  'NAME': os.environ.get('DATABASE_NAME', 'startupwebapp_test'),
                  'USER': os.environ.get('DATABASE_USER', 'postgres'),
                  'PASSWORD': os.environ.get('DATABASE_PASSWORD', 'postgres'),
                  'HOST': os.environ.get('DATABASE_HOST', 'localhost'),
                  'PORT': os.environ.get('DATABASE_PORT', '5432'),
                  'CONN_MAX_AGE': 600,
              }
          }

          # Stripe test keys (placeholder - tests mock Stripe anyway)
          STRIPE_SERVER_SECRET_KEY = 'sk_test_ci_placeholder'
          STRIPE_PUBLISHABLE_SECRET_KEY = 'pk_test_ci_placeholder'
          STRIPE_LOG_LEVEL = 'debug'

          # CORS - allow all for testing
          CORS_ORIGIN_WHITELIST = (
              'http://localhost:8080',
              'http://frontend',
              'http://testserver',
          )
          CORS_ALLOW_CREDENTIALS = True
          EOF

      # ----------------------------------------------------------------------
      # STEP 5: Run linting
      # ----------------------------------------------------------------------
      # Check code quality before running tests
      - name: Run flake8 linting
        if: ${{ github.event.inputs.skip_unit_tests != 'true' }}
        run: |
          cd StartupWebApp
          flake8 user order clientevent StartupWebApp --max-line-length=120 --exclude=*/migrations/* --statistics

      # ----------------------------------------------------------------------
      # STEP 6: Run unit tests
      # ----------------------------------------------------------------------
      # Run 712 unit tests in parallel (faster execution)
      - name: Run unit tests
        if: ${{ github.event.inputs.skip_unit_tests != 'true' }}
        env:
          # Django uses these environment variables
          DATABASE_ENGINE: postgresql
          DATABASE_NAME: startupwebapp_test
          DATABASE_USER: postgres
          DATABASE_PASSWORD: postgres
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
        run: |
          cd StartupWebApp
          python manage.py test order.tests user.tests clientevent.tests StartupWebApp.tests --parallel=4

      # ----------------------------------------------------------------------
      # STEP 7: Set up Firefox for functional tests
      # ----------------------------------------------------------------------
      # Functional tests use Selenium with Firefox
      # Note: Ubuntu 24.04 (GitHub Actions) ships Firefox as snap, not firefox-esr package
      - name: Set up Firefox
        run: |
          # Firefox is pre-installed as snap on GitHub Actions Ubuntu 24.04
          # Verify it's available and show version
          firefox --version || sudo snap install firefox
          firefox --version

      # ----------------------------------------------------------------------
      # STEP 8: Install geckodriver
      # ----------------------------------------------------------------------
      # Selenium needs geckodriver to control Firefox
      - name: Install geckodriver
        run: |
          wget https://github.com/mozilla/geckodriver/releases/download/v0.33.0/geckodriver-v0.33.0-linux64.tar.gz
          tar -xzf geckodriver-v0.33.0-linux64.tar.gz
          sudo mv geckodriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/geckodriver

      # ----------------------------------------------------------------------
      # STEP 9: Clone frontend repository
      # ----------------------------------------------------------------------
      # Functional tests require frontend HTML/JS files to be served
      - name: Clone frontend repository
        run: |
          # Clone to /tmp which is world-readable (nginx can access)
          # Cloning to ~/work fails because nginx can't traverse /home/runner path
          cd /tmp
          git clone https://github.com/bartgottschalk/startup_web_app_client_side.git frontend

          # Make frontend files readable by nginx (nginx runs as www-data user)
          sudo chown -R www-data:www-data frontend
          echo "Frontend cloned to /tmp/frontend and ownership changed to www-data"

      # ----------------------------------------------------------------------
      # STEP 10: Install and configure nginx
      # ----------------------------------------------------------------------
      # Use nginx to serve frontend files (same as Docker environment)
      - name: Install and configure nginx
        run: |
          # Install nginx
          sudo apt-get update
          sudo apt-get install -y nginx

          # Copy nginx config from backend repo
          sudo cp nginx.conf /etc/nginx/sites-available/startupwebapp

          # Update nginx config:
          # 1. Change root directory to /tmp/frontend (world-readable location)
          # 2. Change listen port from 80 to 8080 (port 80 requires root, 8080 is safer)
          sudo sed -i 's|/usr/share/nginx/html|/tmp/frontend|g' /etc/nginx/sites-available/startupwebapp
          sudo sed -i 's|listen 80;|listen 8080;|g' /etc/nginx/sites-available/startupwebapp

          # Enable site and remove default
          sudo ln -sf /etc/nginx/sites-available/startupwebapp /etc/nginx/sites-enabled/
          sudo rm -f /etc/nginx/sites-enabled/default

          # Test nginx configuration
          sudo nginx -t

          # Setup /etc/hosts BEFORE testing nginx (needed for hostname resolution)
          # Functional tests need these hostnames to resolve to localhost for cookie sharing
          echo "127.0.0.1    localliveservertestcase.startupwebapp.com" | sudo tee -a /etc/hosts
          echo "127.0.0.1    localliveservertestcaseapi.startupwebapp.com" | sudo tee -a /etc/hosts
          echo "/etc/hosts configured:"
          cat /etc/hosts | grep localliveservertestcase

          # Restart nginx to load new configuration (in case nginx was already running)
          sudo systemctl restart nginx
          sudo systemctl status nginx --no-pager

          # Verify nginx is serving files on port 8080 - FAIL FAST if not working
          sleep 2
          echo "Verifying nginx is serving files..."

          # Show active nginx config for debugging
          echo "Active nginx config:"
          sudo cat /etc/nginx/sites-enabled/startupwebapp | head -20

          # Test root page with localhost
          if ! curl -s http://localhost:8080/ > /dev/null; then
            echo "❌ ERROR: Frontend server not responding on port 8080"
            exit 1
          fi
          echo "✓ Frontend server responding on localhost:8080"

          # Test with the actual hostname that Selenium will use
          if ! curl -s http://localliveservertestcase.startupwebapp.com:8080/ > /dev/null; then
            echo "❌ ERROR: Frontend server not responding on localliveservertestcase.startupwebapp.com:8080"
            echo "Testing DNS resolution:"
            ping -c 1 localliveservertestcase.startupwebapp.com
            echo "Testing with curl verbose:"
            curl -v http://localliveservertestcase.startupwebapp.com:8080/
            exit 1
          fi
          echo "✓ Frontend server responding on localliveservertestcase.startupwebapp.com:8080"

          # Test specific pages that functional tests use
          if ! curl -s http://localliveservertestcase.startupwebapp.com:8080/about > /dev/null; then
            echo "❌ ERROR: /about page not found"
            curl -v http://localliveservertestcase.startupwebapp.com:8080/about
            exit 1
          fi
          echo "✓ /about page accessible"

          if ! curl -s http://localliveservertestcase.startupwebapp.com:8080/products > /dev/null; then
            echo "❌ ERROR: /products page not found"
            curl -v http://localliveservertestcase.startupwebapp.com:8080/products
            exit 1
          fi
          echo "✓ /products page accessible"

          # Check if the HTML contains expected title tags (not 403 Forbidden)
          echo "Checking HTML content for page titles..."
          ABOUT_TITLE=$(curl -s http://localliveservertestcase.startupwebapp.com:8080/about | grep -i "<title>")
          echo "About page title: $ABOUT_TITLE"
          if echo "$ABOUT_TITLE" | grep -q "403 Forbidden"; then
            echo "❌ ERROR: nginx returning 403 Forbidden - permission issue"
            ls -la /tmp/frontend | head -20
            exit 1
          fi

          PRODUCTS_TITLE=$(curl -s http://localliveservertestcase.startupwebapp.com:8080/products | grep -i "<title>")
          echo "Products page title: $PRODUCTS_TITLE"
          if echo "$PRODUCTS_TITLE" | grep -q "403 Forbidden"; then
            echo "❌ ERROR: nginx returning 403 Forbidden - permission issue"
            ls -la /tmp/frontend | head -20
            exit 1
          fi

          echo "✓ All frontend pages verified successfully"

      # ----------------------------------------------------------------------
      # STEP 11: Run functional tests
      # ----------------------------------------------------------------------
      # Run 28 functional tests with Selenium (headless mode)
      - name: Run functional tests
        env:
          DATABASE_ENGINE: postgresql
          DATABASE_NAME: startupwebapp_test
          DATABASE_USER: postgres
          DATABASE_PASSWORD: postgres
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
          HEADLESS: "TRUE"
          CI_ENV: "true"  # Tell tests frontend is on port 8080, backend on port 60767
        run: |
          # /etc/hosts already configured in previous step
          cd StartupWebApp
          # Run without --parallel flag (Django 4.2 default is serial execution)
          # This ensures LiveServerTestCase instances don't conflict on port 60767
          python manage.py test functional_tests

  # ==========================================================================
  # JOB 2: Build and Push Docker Image
  # ==========================================================================
  #
  # PURPOSE: Create production Docker image and upload to AWS ECR
  # RUNS ON: Ubuntu latest
  # DEPENDS ON: test job must succeed first
  # DURATION: ~3-5 minutes
  #
  # WHY: We need the latest code in a Docker image so ECS can run it.
  # The image is pushed to ECR (Elastic Container Registry), AWS's Docker Hub.
  #
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    # needs: This job waits for 'test' job to complete successfully
    # If tests fail, this job never runs (fail-fast approach)
    needs: test
    # If tests were skipped, still run this job
    if: always() && (needs.test.result == 'success' || needs.test.result == 'skipped')

    # Outputs: Variables this job produces for use by other jobs
    outputs:
      image: ${{ steps.build-image.outputs.image }}

    steps:
      # ----------------------------------------------------------------------
      # STEP 1: Checkout code
      # ----------------------------------------------------------------------
      - name: Checkout code
        uses: actions/checkout@v4

      # ----------------------------------------------------------------------
      # STEP 2: Configure AWS credentials
      # ----------------------------------------------------------------------
      # Sets up AWS CLI with credentials from GitHub Secrets
      # This allows us to push to ECR and trigger ECS tasks
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # ----------------------------------------------------------------------
      # STEP 3: Login to Amazon ECR
      # ----------------------------------------------------------------------
      # Gets temporary Docker credentials for pushing images to ECR
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      # ----------------------------------------------------------------------
      # STEP 4: Build Docker image
      # ----------------------------------------------------------------------
      # Builds production image using multi-stage Dockerfile
      # Tags it with git commit SHA for traceability
      - name: Build Docker image
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build production target from Dockerfile
          docker build \
            --target production \
            --tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
            --tag $ECR_REGISTRY/$ECR_REPOSITORY:latest \
            --file Dockerfile \
            .

          # Push both tags (commit SHA and 'latest')
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest

          # Output the full image URI for next job
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

      # ----------------------------------------------------------------------
      # STEP 5: Display image info
      # ----------------------------------------------------------------------
      # Shows image size and tags (helpful for debugging)
      - name: Display image info
        run: |
          echo "Image built and pushed successfully:"
          echo "- Repository: ${{ env.ECR_REPOSITORY }}"
          echo "- Tag: ${{ github.sha }}"
          echo "- Full URI: ${{ steps.build-image.outputs.image }}"

  # ==========================================================================
  # JOB 3: Run Database Migrations
  # ==========================================================================
  #
  # PURPOSE: Execute Django migrations on AWS RDS via ECS Fargate
  # RUNS ON: Ubuntu latest
  # DEPENDS ON: build-and-push job must succeed first
  # DURATION: ~2-5 minutes
  #
  # WHY: This is the main goal - update the database schema on production.
  # We run migrations as an ECS task (serverless container) which:
  # 1. Pulls the Docker image from ECR
  # 2. Connects to RDS with credentials from Secrets Manager
  # 3. Runs "python manage.py migrate"
  # 4. Logs everything to CloudWatch
  #
  run-migrations:
    name: Run Migrations on ${{ github.event.inputs.database }}
    runs-on: ubuntu-latest
    needs: build-and-push

    steps:
      # ----------------------------------------------------------------------
      # STEP 1: Checkout code
      # ----------------------------------------------------------------------
      - name: Checkout code
        uses: actions/checkout@v4

      # ----------------------------------------------------------------------
      # STEP 2: Configure AWS credentials
      # ----------------------------------------------------------------------
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # ----------------------------------------------------------------------
      # STEP 3: Get ECS task definition
      # ----------------------------------------------------------------------
      # Downloads the current task definition from AWS
      - name: Get ECS task definition
        id: get-task-def
        run: |
          aws ecs describe-task-definition \
            --task-definition ${{ env.ECS_TASK_DEFINITION }} \
            --query 'taskDefinition' \
            --output json > task-definition.json

          # Display for debugging
          echo "Task definition retrieved"

      # ----------------------------------------------------------------------
      # STEP 4: Update task definition with new image
      # ----------------------------------------------------------------------
      # Modifies the task definition to use the newly built Docker image
      - name: Update task definition with new image
        id: update-task-def
        run: |
          # Extract the current task definition and update image
          TASK_DEF=$(cat task-definition.json)

          # Update the image URI and environment variables
          UPDATED_TASK_DEF=$(echo $TASK_DEF | jq \
            --arg IMAGE "${{ needs.build-and-push.outputs.image }}" \
            --arg DB_NAME "${{ env.DATABASE_NAME }}" \
            '
            .containerDefinitions[0].image = $IMAGE |
            .containerDefinitions[0].environment |= map(
              if .name == "DATABASE_NAME" then .value = $DB_NAME else . end
            )
            ')

          # Remove fields that aren't needed for registration
          CLEAN_TASK_DEF=$(echo $UPDATED_TASK_DEF | jq '
            del(
              .taskDefinitionArn,
              .revision,
              .status,
              .requiresAttributes,
              .compatibilities,
              .registeredAt,
              .registeredBy
            )
          ')

          echo "$CLEAN_TASK_DEF" > updated-task-definition.json

          # Register new task definition revision
          TASK_DEF_ARN=$(aws ecs register-task-definition \
            --cli-input-json file://updated-task-definition.json \
            --query 'taskDefinition.taskDefinitionArn' \
            --output text)

          echo "task-def-arn=$TASK_DEF_ARN" >> $GITHUB_OUTPUT
          echo "Registered new task definition: $TASK_DEF_ARN"

      # ----------------------------------------------------------------------
      # STEP 5: Run ECS task
      # ----------------------------------------------------------------------
      # Launches the migration task on ECS Fargate
      - name: Run ECS migration task
        id: run-task
        run: |
          # Get VPC configuration from existing resources
          SUBNETS=$(aws ec2 describe-subnets \
            --filters "Name=tag:Name,Values=startupwebapp-private-subnet-*" \
            --query 'Subnets[*].SubnetId' \
            --output text | tr '\t' ',')

          SECURITY_GROUP=$(aws ec2 describe-security-groups \
            --filters "Name=tag:Name,Values=startupwebapp-backend-sg" \
            --query 'SecurityGroups[0].GroupId' \
            --output text)

          # Launch the task
          TASK_ARN=$(aws ecs run-task \
            --cluster ${{ env.ECS_CLUSTER }} \
            --task-definition ${{ steps.update-task-def.outputs.task-def-arn }} \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[$SUBNETS],securityGroups=[$SECURITY_GROUP],assignPublicIp=DISABLED}" \
            --query 'tasks[0].taskArn' \
            --output text)

          echo "task-arn=$TASK_ARN" >> $GITHUB_OUTPUT
          echo "Migration task launched: $TASK_ARN"

      # ----------------------------------------------------------------------
      # STEP 6: Wait for task completion
      # ----------------------------------------------------------------------
      # Polls ECS until the task finishes (success or failure)
      - name: Wait for task to complete
        id: wait-task
        run: |
          echo "Waiting for migration task to complete..."

          # Wait up to 10 minutes for task to finish
          aws ecs wait tasks-stopped \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks ${{ steps.run-task.outputs.task-arn }} \
            --max-attempts 60 \
            --delay 10

          # Get task exit code
          EXIT_CODE=$(aws ecs describe-tasks \
            --cluster ${{ env.ECS_CLUSTER }} \
            --tasks ${{ steps.run-task.outputs.task-arn }} \
            --query 'tasks[0].containers[0].exitCode' \
            --output text)

          echo "exit-code=$EXIT_CODE" >> $GITHUB_OUTPUT
          echo "Task completed with exit code: $EXIT_CODE"

      # ----------------------------------------------------------------------
      # STEP 7: Fetch CloudWatch logs
      # ----------------------------------------------------------------------
      # Downloads migration logs from CloudWatch for review
      - name: Fetch migration logs
        if: always()
        run: |
          echo "Fetching CloudWatch logs..."

          # Extract task ID from ARN
          TASK_ID=$(echo "${{ steps.run-task.outputs.task-arn }}" | awk -F'/' '{print $NF}')

          # Get logs from CloudWatch
          aws logs get-log-events \
            --log-group-name /ecs/startupwebapp-migrations \
            --log-stream-name migration/$TASK_ID \
            --query 'events[*].message' \
            --output text || echo "No logs available yet"

      # ----------------------------------------------------------------------
      # STEP 8: Check success
      # ----------------------------------------------------------------------
      # Fails the workflow if migrations failed
      - name: Check migration success
        if: always()
        run: |
          if [ "${{ steps.wait-task.outputs.exit-code }}" != "0" ]; then
            echo "❌ Migration failed with exit code ${{ steps.wait-task.outputs.exit-code }}"
            exit 1
          else
            echo "✅ Migration completed successfully!"
          fi

  # ==========================================================================
  # JOB 4: Notify on Completion
  # ==========================================================================
  #
  # PURPOSE: Send summary of workflow results
  # RUNS ON: Ubuntu latest
  # RUNS: Always (even if previous jobs fail)
  # DURATION: ~10 seconds
  #
  notify:
    name: Workflow Summary
    runs-on: ubuntu-latest
    needs: [test, build-and-push, run-migrations]
    if: always()

    steps:
      - name: Display workflow summary
        run: |
          echo "=========================================="
          echo "Migration Workflow Summary"
          echo "=========================================="
          echo "Database: ${{ github.event.inputs.database }}"
          echo "Tests: ${{ needs.test.result }}"
          echo "Build: ${{ needs.build-and-push.result }}"
          echo "Migrations: ${{ needs.run-migrations.result }}"
          echo "=========================================="

          if [ "${{ needs.run-migrations.result }}" == "success" ]; then
            echo "✅ Migrations applied successfully to ${{ github.event.inputs.database }}"
          else
            echo "❌ Migration workflow failed - check logs above"
            exit 1
          fi
